{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rap-lyric-generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMCDo3DRNA/Rvj4TrcOwuxx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janakanchandru/rap-lyric-generator/blob/master/rap_lyric_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxnUmV_2THoy",
        "colab_type": "text"
      },
      "source": [
        "# Rap Lyrics Generator\n",
        "## This file is for training (I like free GPUs)\n",
        "Imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GmNQRnIE60X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from random import shuffle\n",
        "from tqdm import tqdm\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}, to remove tf console output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH2PEgLSVIdw",
        "colab_type": "text"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbvhV5kAS49z",
        "colab_type": "text"
      },
      "source": [
        "Mappings used to split dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIsLws7GPzWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_seq = chunk[:-1]\n",
        "    target = chunk[-1]\n",
        "    return input_seq, target\n",
        "\n",
        "def make_target_categorical(X, y):\n",
        "    vocabSet_size = 7016 # unfortunatley this needs to be hardcoded, can't find a dynamic way\n",
        "    y = tf.one_hot(y, vocabSet_size)\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAqbBPGPTV-E",
        "colab_type": "text"
      },
      "source": [
        "Function to convert encoded examples to words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUejv8HLTTGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_word(X, y_idx, idx_not_in_vocabSet, idx2word):\n",
        "    word_X = ''\n",
        "    word_y = ''\n",
        "\n",
        "    for idx in X:\n",
        "        if idx == idx_not_in_vocabSet:\n",
        "            word_X += 'n/a '\n",
        "        else:\n",
        "            word_X += idx2word[idx] + ' '\n",
        "\n",
        "    if y_idx == idx_not_in_vocabSet:\n",
        "        word_y = 'n/a (word not in vocabSet)'\n",
        "    else:\n",
        "        word_y = idx2word[y_idx]\n",
        "\n",
        "    return word_X, word_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8feb7YVTeiI",
        "colab_type": "text"
      },
      "source": [
        "Function to process the data and create a TF dataset object for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDSYb3dxGVIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(encoded_train_set_filename, vocab_set_filename, word2idx_filename, idx2word_filename, \n",
        "                seq_length, batch_size, num_examples_to_use=0):\n",
        "    # get training data\n",
        "    songData = []\n",
        "    with open(encoded_train_set_filename, 'r') as f:\n",
        "        for song in f.read().splitlines():\n",
        "            song_split = song.split()\n",
        "            add = []\n",
        "            for num in song_split:\n",
        "                add.append(int(num))\n",
        "            songData.append(add)\n",
        "\n",
        "    # load vocabSet\n",
        "    vocabSet = []\n",
        "    with open(vocab_set_filename, 'r') as f:\n",
        "        vocabSet = f.read().splitlines()\n",
        "\n",
        "    vocabSet_size = len(vocabSet) + 1\n",
        "    idx_not_in_vocabSet = len(vocabSet)\n",
        "\n",
        "    # load lookup tables\n",
        "    word2idx = np.load('data/word2idx.npy', allow_pickle=True).item()\n",
        "    idx2word = np.load('data/idx2word.npy', allow_pickle=True)\n",
        "\n",
        "    # split lyrics into segments of len = seq_length\n",
        "    print('Splitting songs into sequences of seq_length...')\n",
        "    sequences = []\n",
        "    for song in tqdm(songData):\n",
        "        for i in range(seq_length+1, len(song)):\n",
        "            sequences.append(song[i - (seq_length+1) : i])\n",
        "    if num_examples_to_use:\n",
        "        sequences = sequences[:num_examples_to_use]\n",
        "    \n",
        "    # create TF dataset, allows for large datasets without memory issues\n",
        "    print('\\nCreating TF dataset object out of sequences, this may take awhile...')\n",
        "    print('MAKE SURE you input the correct vocabSet size in make_target_categorical(): ', vocabSet_size)\n",
        "    sequences_dataset = tf.data.Dataset.from_tensor_slices(sequences)\n",
        "    dataset = sequences_dataset.map(split_input_target)\n",
        "    dataset = dataset.map(make_target_categorical)\n",
        "\n",
        "    print('\\nSample input and ouput: ')\n",
        "    for i, o in dataset.take(1):\n",
        "        X = i.numpy()\n",
        "        y = o.numpy()\n",
        "        y_idx = np.argmax(y, axis=-1)\n",
        "        word_X, word_y = convert_to_word(X, y_idx, idx_not_in_vocabSet, idx2word)\n",
        "\n",
        "        print('input: {}'.format(X))\n",
        "        print('output shape: {}'.format(y.shape))\n",
        "        print('output idx: {}'.format(y_idx))\n",
        "        print('\\ninput word sequence: {}'.format(word_X))\n",
        "        print('output word: {}'.format(word_y))\n",
        "\n",
        "    buffer_size = 10000\n",
        "    dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    print('\\nTotal # of training examples: ', len(sequences))\n",
        "\n",
        "    print('\\nDataset created')\n",
        "\n",
        "    return vocabSet, word2idx, idx2word, dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wduP97IIT1ZP",
        "colab_type": "text"
      },
      "source": [
        "## Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS3xx9lXGYQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "class ModelV1:\n",
        "    def __init__(self):\n",
        "        self.SEQ_LENGTH = 100\n",
        "        self.BATCH_SIZE = 128\n",
        "        self.EMBEDDING_DIM = 256\n",
        "        self.RNN_UNITS = 512\n",
        "\n",
        "    def build_model(self, vocab_size):\n",
        "        model = tf.keras.Sequential([\n",
        "            Embedding(vocab_size, self.EMBEDDING_DIM, input_length=self.SEQ_LENGTH),\n",
        "\n",
        "            LSTM(self.RNN_UNITS, return_sequences=True, stateful=False, recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "            LSTM(self.RNN_UNITS, return_sequences=False, stateful=False, recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "            Dense(vocab_size, activation='softmax')\n",
        "        ])\n",
        "        model.summary()\n",
        "\n",
        "        return model\n",
        "\n",
        "class ModelV2:\n",
        "    def __init__(self):\n",
        "        self.SEQ_LENGTH = 100\n",
        "        self.BATCH_SIZE = 128\n",
        "        self.EMBEDDING_DIM = 256\n",
        "        self.RNN_UNITS = 512\n",
        "\n",
        "    def build_model(self, vocab_size):\n",
        "        model = tf.keras.Sequential([\n",
        "            Embedding(vocab_size, self.EMBEDDING_DIM, input_length=self.SEQ_LENGTH),\n",
        "\n",
        "            Bidirectional(LSTM(self.RNN_UNITS, \n",
        "                                return_sequences=True,\n",
        "                                stateful=False,\n",
        "                                recurrent_initializer='glorot_uniform')),\n",
        "\n",
        "            Bidirectional(LSTM(self.RNN_UNITS, \n",
        "                                return_sequences=False,\n",
        "                                stateful=False,\n",
        "                                recurrent_initializer='glorot_uniform')),\n",
        "\n",
        "            Dense(vocab_size, activation='softmax')\n",
        "        ])\n",
        "        model.summary()\n",
        "\n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmKhuwhhVzk9",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKLJDppnGdgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constants\n",
        "EPOCHS = 24\n",
        "# NUM_EXAMPLES_TO_USE = 100000 # if you want to limit the number of examples used for training\n",
        "TRAIN_SET_FILENAME = 'data/encoded_train_val_set.txt'\n",
        "VOCABSET_FILENAME = 'data/vocabSet.txt'\n",
        "WORD2IDX_FILENAME = 'data/word2idx.npy'\n",
        "IDX2WORD_FILENAME = 'data/idx2word.npy'\n",
        "\n",
        "# prepare checkpoint directory\n",
        "checkpoint_dir = 'results'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTTavXjDGrYg",
        "colab_type": "text"
      },
      "source": [
        "Modify below if you want to train ModelV2 (bidirectional LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvwUfELmV5ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initate model\n",
        "modelv1 = ModelV1()\n",
        "params = [modelv1.EMBEDDING_DIM, modelv1.RNN_UNITS, modelv1.SEQ_LENGTH, modelv1.BATCH_SIZE]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH3NNY2dWEI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare data\n",
        "vocabSet, word2idx, idx2word, dataset = preprocess(TRAIN_SET_FILENAME, \n",
        "                                                    VOCABSET_FILENAME,\n",
        "                                                    WORD2IDX_FILENAME,\n",
        "                                                    IDX2WORD_FILENAME,\n",
        "                                                    params[2], \n",
        "                                                    params[3])\n",
        "                                                    # NUM_EXAMPLES_TO_USE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLsbLooiWJU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build model\n",
        "model = modelv1.build_model(len(vocabSet)+1)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model.load_weights(os.path.join('data', 'ckpt_26')) #can be useful to train in batches so collab doesn't recycle the runtime before full training completes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_zcUkckWVgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train model\n",
        "history = model.fit(dataset, batch_size=params[3], epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SgCZAXwWVkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save model\n",
        "model.save('results/rapper_bidirectional.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOoZv_q4WdDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(history.history)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}